from tqdm import tqdm
import time
from typing import Callable, List, Tuple

import openai
from torch.utils.data import Dataset

from evaluation.codex import codex
from execution.execution_evaluation import (
    execution_eval_at_k,
)


def codex_evaluate_pass_at_k(
    input_dataset: Dataset,
    prompt: str,
    get_ins_text: Callable,
    get_ins_answer: Callable,
    exec_func: Callable,
    get_ins_few_shot_text: Callable = None,
    get_ins_few_shot_soln: Callable = None,
    eval_at_k: int = 1,
    few_shot_enabled: bool = False,
) -> Tuple[List[List[str]], float, float]:
    """
    Determines accuracy@k and pass@k for a given set of examples.

    Args:
        input_dataset: The dataset to evaluate.
        prompt: The prompt to append before each example input.
        get_ins_text: A function that returns the text of a dataset example, given an example.
        get_ins_answer: A function that returns the answer of a dataset example, given an example.
        get_ins_few_shot_text: A function that returns the few shot text of a dataset example, given an example.
        get_ins_few_shot_soln: A function that returns the few shot solution of a dataset example, given an example.

        exec_func: a function to execute the code generated by the model.

        eval_at_k: the value of k.
        few_shot_enabled: whether to use few shot evaluation.

    Returns:
        A tuple of (programs, accuracy, pass_at_k).
            programs: a list of lists of programs generated by the model. Each list is a batch of programs from a single example.
            accuracy: the avg value of accuracy@k over the dataset.
            pass_at_k: the avg value of pass@k over the dataset.

    """
    programs = []
    running_acc, running_pass_at_k = 0, 0

    with tqdm(total=len(input_dataset) * eval_at_k) as bar:
        for item in input_dataset.instances:

            # prepare input for few-shot experiments if applicable
            if few_shot_enabled is True:
                processed_input = create_few_shot_prompt(
                    prompt, item, get_ins_few_shot_text, get_ins_few_shot_soln
                )

                processed_input = (
                    processed_input + "\n\n" + "\n".join((prompt, get_ins_text(item)))
                )
            else:
                processed_input = "\n\n".join((prompt, get_ins_text(item)))
            # print(processed_input)

            # perform generation with codex eval_at_k times
            progs = []
            for gen in range(eval_at_k):
                while True:
                    try:
                        result = codex(
                            [processed_input],
                            engine="code-davinci-001",
                            temperature=1.0,
                            top_p=1,
                            max_tokens=128,
                        )

                        program = "\n".join((get_ins_text(item), result[0]))
                        progs.append(program)

                        bar.update(1)
                        break
                    except openai.error.RateLimitError as e:

                        print(
                            "RateLimitError occurred, waiting for 60 seconds and retrying..."
                        )
                        time.sleep(60)

            # check execution accuracy and pass at k for the instance
            (
                acc,
                pass_k,
            ) = execution_eval_at_k(  # NOTE: execution_eval_at_k is currently MathQA-specific!!
                progs, exec_func, get_ins_answer(item), eval_at_k
            )

            running_acc += acc
            running_pass_at_k += pass_k

            programs.append(progs)

    avg_acc = running_acc / len(input_dataset.instances)
    avg_pass_at_k = running_pass_at_k / len(input_dataset.instances)

    return programs, avg_acc, avg_pass_at_k


def create_few_shot_prompt(
    prompt, item, get_ins_few_shot_text: Callable, get_ins_few_shot_soln: Callable
):
    """Appends the prompt to each few shot question and its solution, and concatenates these examples"""

    if get_ins_few_shot_text is None or get_ins_few_shot_soln is None:
        raise ValueError(
            "few-shot text and soln retrieval functions must be provided. See documentation for more info."
        )

    processed_input = ""
    for text, code in zip(get_ins_few_shot_text(item), get_ins_few_shot_soln(item)):

        if processed_input != "":

            processed_input = processed_input + "\n\n"

        processed_input = processed_input + "\n".join((prompt, text, code))

    # processed_input = processed_input + "\n\n"

    return processed_input


def mathqa_ins_text(item):
    return item["metadata"]["text"]


def mathqa_ins_answer(item):
    return item["metadata"]["answer"]


def mathqa_ins_few_shot_text(item):
    return item["metadata"]["few_shot_text"]


def mathqa_ins_few_shot_soln(item):
    return item["metadata"]["few_shot_code"]
