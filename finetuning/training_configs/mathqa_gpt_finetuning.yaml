seed_everything: 333
trainer:
  # gpus: 2
  gradient_clip_val: 1.0
  default_root_dir: debug-tmp
  val_check_interval: 1.0
  max_steps: &max_steps 100000
  # progress_bar_refresh_rate: 1
  log_every_n_steps: 1
  # logger:
  #   - class_path: lightning_modules.loggers.patched_loggers.PatchedCSVLogger
  #     init_args:
  #       # save_dir: logs/csv
  #       name: csv
  #       # version: 0
  #   - class_path: lightning_modules.loggers.patched_loggers.PatchedTensorBoardLogger
  #     init_args:
  #       # save_dir: logs/tb
  #       name: tb
  #       # version: 0
    # - class_path: lightning_modules.loggers.patched_loggers.PatchedNeptuneLogger
    #   init_args:
    #     project_name: ansong.ni/trace-codegen
  callbacks:
    - class_path: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
      init_args:
        monitor: exec_acc
        mode: max
        filename: '{step}-{exec_acc:.4f}-{exec_rate:.4f}'
        save_top_k: 5
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: pytorch_lightning.callbacks.progress.TQDMProgressBar
      init_args:
        refresh_rate: 1
    # - class_path: pytorch_lightning.callbacks.gpu_stats_monitor.GPUStatsMonitor
    #   init_args:
    #     memory_utilization: true
    #     gpu_utilization: true
        
  accelerator: gpu
  devices: 2
  # replace_sampler_ddp: False
  # https://github.com/PyTorchLightning/pytorch-lightning/issues/8262
  # plugins: "ddp_find_unused_parameters_false"
  strategy: deepspeed_stage_3
  # strategy: 
  #   class_path: pytorch_lightning.plugins.DeepSpeedPlugin
  #   init_args:
  #     stage: 3
  #     offload_optimizer: true
  #     offload_parameters: true
  #     overlap_comm: true
  #     allgather_bucket_size: 700000000
  #     reduce_bucket_size: 700000000
  #     params_buffer_size: 100000000
  #     max_in_cpu: 100000000
  #     pin_memory: true
  #     contiguous_gradients: true
  precision: 16
  # accumulate_grad_batches: 4

  #  - class_path: lightning_modules.callbacks.save_prediction_callback.SavePredictionCallback
model:
  class_path: lightning_modules.models.gpt_seq2seq_model.GptSeq2SeqModel
  init_args:
    transformer_model_name: &transformer EleutherAI/gpt-neo-1.3B
    max_gen_len: 256
    sampling_temp: 0.2
    # sampling_temp_at_k: 0.8
    # pass_at_k: 80
    # eval_pass_at_k_every_n_epochs: 1
    # max_generation_batches: 10
    gradient_ckpt: true
    # eval_greedy_search: true

data:
  class_path: lightning_modules.datasets.mathqa_reader.MathQADataModule
  init_args:
    transformer_model_name: *transformer
    batch_size: 1
    val_batch_size: 4
    train_file_path: data/mathqa/train-python.jsonl
    val_file_path: data/mathqa/val_python_with_states.jsonl
    train_max_instances: 20
    val_max_instances: 40
    # few_shot_n: 4
optimizer:
  class_path: torch.optim.adamw.AdamW
  init_args: 
    lr: 1.0e-4
    betas: 
      - 0.9
      - 0.95
    eps: 1.0e-8
    weight_decay: 0.1
