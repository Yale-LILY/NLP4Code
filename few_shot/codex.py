import openai
import os
import time 
import json
import numpy as np

from tqdm import tqdm
from typing import List, Optional, Dict, Any, Callable, Tuple
from execution.execution_evaluation import batch_eval_at_k

OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
if OPENAI_API_KEY is None:
    raise Exception("Please set your OpenAI API key in the environment variable OPENAI_API_KEY")
else:
    openai.api_key = OPENAI_API_KEY

# From the Codex Paper
def estimate_pass_at_k(n, c, k):
    """
    :param n: total number of samples
    :param c: number of correct samples
    :param k: k in pass@$k$
    """
    if n - c < k: 
        return 1.0
    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))

def openai_call(prompts: List[str], engine: str, max_tokens: int = 1024, temperature: Optional[float] = None, 
             top_p: Optional[float] = None, n: Optional[int] = None, best_of: Optional[int] = None, 
             wait_on_limit: bool = True, **kwargs) -> List[List[str]]:
    """
    Takes a list of prompts and returns a list of generated responses for each prompt.
    
    For more arguments to https://beta.openai.com/docs/api-reference/completions/create
    """
    # prune out the None arguments since openAI api treats unspecified arguments differently
    def prune_none_args(**kwargs):
        return {k: v for k, v in kwargs.items() if v is not None}

    while True:
        try:
            completion = openai.Completion.create(**prune_none_args(engine=engine, prompt=prompts, max_tokens=max_tokens, 
                                                temperature=temperature, top_p=top_p, n=n, best_of=best_of, **kwargs))
            break
        except openai.error.RateLimitError as e:
            print(e)
            if wait_on_limit:
                print("RateLimitError occurred, waiting for 30 seconds and retrying...")
                time.sleep(30)
            else:
                raise e
        except openai.error.APIError as e:
            print("APIError occurred, retry in 5 seconds...")
            time.sleep(5)
    # get the text from the returned results and slice the completions to input_n * completion_n
    completion_texts = [x.text for x in completion.choices]
    completion_results = [completion_texts[i*n:(i+1)*n] for i in range(len(prompts))] 

    return completion_results

def codex(prompts: List[str], engine: str="code-davinci-001", max_tokens: int=1024, **kwargs) -> List[List[str]]:
    return openai_call(prompts, engine, max_tokens, **kwargs)

def gpt3(prompts: List[str], engine: str="text-davinci-001", max_tokens: int=1024, **kwargs) -> List[List[str]]:
    return openai_call(prompts, engine, max_tokens, **kwargs)

def codex_evaluate_pass_at_k(
    few_shot_examples: List[Dict[str, Any]],
    input_dataset: List[Dict[str, Any]],
    text_header: str,
    promptify_func: Callable[[Dict[str, Any], bool], str], # the bool is to specify whether to include the sol
    exec_func: Callable,
    answer_eq_func: Callable,
    prepare_extra_exec_args_func: Callable = None,
    eval_at_ks: List[int] = [1],
    openai_kwargs: Dict[str, Any] = {},
    save_result_path: str = None,
    batch_prompts: int = 20,
) -> Tuple[List[List[str]], List[float]]:
    """
    Determines accuracy@k and pass@k for a given set of examples.

    Args:
        input_dataset: The dataset to evaluate.
        text_header: The header to prepend to the examples, typically the test description
        promptify_func: given the example, returns the prompt to use for the example
        exec_func: a function to execute the code generated by the model.

        eval_at_k: the value of ks for pass_at_k numbers.

    Returns:
        A tuple of (programs, pass_at_k).
            programs: a list of lists of programs generated by the model. Each list is a batch of programs from a single example.
            pass_at_k: a list of pass_at_ks for different values of k

    """
    # NOTE: here we use the max(eval_at_ks) to determine the number of programs to generate, but use the formula 
    # in the codex paper to estimate the smaller pass_at_ks.
    eval_at_k = max(eval_at_ks)

    # set some default kwargs for openai api call
    if "engine" not in openai_kwargs:
        openai_kwargs["engine"] = "code-davinci-001"
    if "max_tokens" not in openai_kwargs:
        openai_kwargs["max_tokens"] = 256
    if "n" not in openai_kwargs:
        openai_kwargs["n"] = eval_at_k
    else:
        assert openai_kwargs["n"] == eval_at_k, "n must be equal to eval_at_k"

    # first we build the prompt with the few-shot examples
    common_prompt = text_header + "\n\n"
    for example in few_shot_examples:
        common_prompt += promptify_func(example, True) + "\n"

    # then we generate the programs for each example
    generated_programs_list = []
    batched_prompts = []
    for example in tqdm(input_dataset):
        example_prompt = common_prompt + promptify_func(example, False)
        batched_prompts.append(example_prompt)

        if len(batched_prompts) == batch_prompts or \
            len(batched_prompts) + len(generated_programs_list) == len(input_dataset): # last batch
            generated_programs: List[List[str]] = openai_call(prompts=batched_prompts, **openai_kwargs)
            batched_prompts = []
            # FIXME: this only works with programs that never has an empty line
            generated_programs = [[program.split("\n\n")[0] for program in programs] for programs in generated_programs] 
            generated_programs_list.extend(generated_programs)
    
    # now we batch evaluate the programs
    answers = [example["answer"] for example in input_dataset] # FIXME: this is a hack and won't work for anything but mathqa
    if prepare_extra_exec_args_func is not None:
        extra_exec_args = prepare_extra_exec_args_func(input_dataset)
    else:
        extra_exec_args = {}

    grouped_raw_eval_results: List[List[bool]] = batch_eval_at_k(generated_programs_list, exec_func, answers, eval_at_k, 
                                                answer_eq_func, extra_exec_args=extra_exec_args)
    grouped_eval_results = [sum(result) for result in grouped_raw_eval_results]

    # get pass at k and for various ks
    pass_at_k_dict = {}
    for k in eval_at_ks:
        if k == max(eval_at_ks):
            pass_at_k_dict[f"pass@{k}"] = [1.0 if result > 0 else 0.0 for result in grouped_eval_results]
        else:
            pass_at_k_dict[f"pass@{k}"] = [estimate_pass_at_k(eval_at_k, result, k) for result in grouped_eval_results]
    
    # print the results
    for k in eval_at_ks:
        print(f"pass@{k}: {np.mean(pass_at_k_dict[f'pass@{k}'])}")

    if save_result_path is not None:
        # make sure that the file doesn't exist
        if not open(save_result_path, "x"):
            raise ValueError(f"File {save_result_path} already exists")
        
        with open(save_result_path, "w") as f:
            save_dicts = []
            for i, result in enumerate(grouped_eval_results):
                save_dict = {"example": input_dataset[i], "generated_programs": generated_programs_list[i], 
                             f"acc@{eval_at_k}": result}
                for k in eval_at_ks:
                    save_dict[f"pass@{k}"] = pass_at_k_dict[f"pass@{k}"][i]
                save_dicts.append(save_dict)
            f.write(json.dumps(save_dicts, indent=2))
    
    return generated_programs_list, grouped_raw_eval_results

if __name__ == "__main__":
    result = openai_call(["# write an addition function\ndef ", "# write a logsumexp function\ndef"], "code-davinci-002", 
                       max_tokens=64, n=2, temperature=0.2, top_p=None, best_of=None)